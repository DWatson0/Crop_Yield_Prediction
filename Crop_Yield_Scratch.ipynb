{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c6984e8-3ffb-40ec-a2eb-6d6b1ddef3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a313c9b-3566-47c7-8ac7-b8907368bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Downloads/Crop Yiled with Soil and Weather.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4993f73-7e18-42c0-98c8-d7e960fde176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fertilizer</th>\n",
       "      <th>temp</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>yeild</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>78.942684</td>\n",
       "      <td>27.014634</td>\n",
       "      <td>76.838312</td>\n",
       "      <td>23.087606</td>\n",
       "      <td>20.044206</td>\n",
       "      <td>10.348503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>79.077362</td>\n",
       "      <td>27.153127</td>\n",
       "      <td>76.927657</td>\n",
       "      <td>23.068234</td>\n",
       "      <td>20.155436</td>\n",
       "      <td>10.528122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>78.986271</td>\n",
       "      <td>28.108948</td>\n",
       "      <td>77.164352</td>\n",
       "      <td>22.863654</td>\n",
       "      <td>20.989871</td>\n",
       "      <td>10.564205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>78.894767</td>\n",
       "      <td>28.141344</td>\n",
       "      <td>76.925149</td>\n",
       "      <td>23.071553</td>\n",
       "      <td>20.873662</td>\n",
       "      <td>10.442511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>79.050307</td>\n",
       "      <td>26.980865</td>\n",
       "      <td>77.124758</td>\n",
       "      <td>23.050197</td>\n",
       "      <td>19.960695</td>\n",
       "      <td>10.584638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2585 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Fertilizer       temp          N          P          K      yeild\n",
       "0      80.000000  28.000000  80.000000  24.000000  20.000000  12.000000\n",
       "1      77.000000  27.000000  78.000000  23.000000  20.000000  12.000000\n",
       "2      80.000000  26.000000  80.000000  24.000000  20.000000  12.000000\n",
       "4      78.000000  27.000000  78.000000  23.000000  19.000000  12.000000\n",
       "6      75.000000  26.000000  75.000000  22.000000  19.000000  12.000000\n",
       "...          ...        ...        ...        ...        ...        ...\n",
       "2591   78.942684  27.014634  76.838312  23.087606  20.044206  10.348503\n",
       "2592   79.077362  27.153127  76.927657  23.068234  20.155436  10.528122\n",
       "2593   78.986271  28.108948  77.164352  22.863654  20.989871  10.564205\n",
       "2594   78.894767  28.141344  76.925149  23.071553  20.873662  10.442511\n",
       "2595   79.050307  26.980865  77.124758  23.050197  19.960695  10.584638\n",
       "\n",
       "[2585 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317ed350-9adc-4a90-b5ea-d8464ff6e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Fertilizer','temp','N','P','K']]\n",
    "y = df['yeild']\n",
    "y = y.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e126d7-beed-43e6-9204-93553337a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4, random_state=3)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_test,y_test,test_size=0.5, random_state=3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_cv_scaled = scaler.fit_transform(X_cv)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7cf3d4-541c-40bd-a255-8bd4fbe39481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b6e6c13-e81a-493f-b476-e7d6435fced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef3aca01-795c-4fdc-b0b2-0bfadb1bc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7203c94f-357a-49ad-8646-5f85561b1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_bias_initialize():\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(X_train_scaled.shape[1],64)*0.1\n",
    "    b1 = np.zeros((1, 64))\n",
    "\n",
    "    W2 = np.random.randn(64,32)*0.1\n",
    "    b2 = np.zeros((1, 32))\n",
    "\n",
    "    W3 = np.random.randn(32, 1)*0.1\n",
    "    b3 = np.zeros((1, 1))\n",
    "\n",
    "    return (W1,b1,W2,b2,W3,b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eb65ea8-cc81-4a6d-9580-99d302ad233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_errors(X, y, X_cv, y_cv, W1, b1, W2, b2, W3, b3):\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = a2 @ W3 + b3\n",
    "    a3 = linear(z3)\n",
    "    trainError = np.mean((a3 - y) ** 2)\n",
    "\n",
    "    z1 = X_cv @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = a2 @ W3 + b3\n",
    "    a3 = linear(z3)\n",
    "    cvError = np.mean((a3 - y_cv) ** 2)\n",
    "\n",
    "    return trainError, cvError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4cbb7fe-abb8-4f19-b710-3df85e8acfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_lambda(_lambda_arr,alpha,X,y,X_cv,y_cv,epochs,patience=3000, min_delta=1e-4):\n",
    "    for _lambda in _lambda_arr:\n",
    "        W1,b1,W2,b2,W3,b3 = weight_bias_initialize()\n",
    "        best_cv_error = float(\"inf\")\n",
    "        best_weights = None\n",
    "        wait = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "    \n",
    "            ##########Forward propagation##########\n",
    "            z1 = X @ W1 + b1 #(2076,5)@(5,64)\n",
    "            a1 = relu(z1)\n",
    "\n",
    "            z2 = a1 @ W2 + b2 # (2076,64)@(64,32)\n",
    "            a2 = relu(z2)\n",
    "\n",
    "            z3 = a2 @ W3 + b3 #(2076,32)@(32,1)\n",
    "            a3 = linear(z3)\n",
    "\n",
    "            error = np.mean((a3 - y)**2 ) + _lambda * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
    "            ##########Backward propagation#############\n",
    "\n",
    "            dz3 = a3 - y\n",
    "            dW3 = a2.T @ dz3 + _lambda * W3\n",
    "            db3 = np.sum(dz3,axis=0,keepdims = True)\n",
    "\n",
    "            da2 = dz3 @ W3.T         \n",
    "            dz2 = da2 * relu_derivative(z2)  \n",
    "            dW2 = a1.T @ dz2 + _lambda * W2        \n",
    "            db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "            da1 = dz2 @ W2.T         \n",
    "            dz1 = da1 * relu_derivative(z1)  \n",
    "            dW1 = X.T @ dz1 + _lambda * W1        \n",
    "            db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "            ##########Update Weights####################\n",
    "            W1 = W1 - alpha*dW1\n",
    "            b1 = b1 - alpha*db1\n",
    "\n",
    "            W2 = W2 - alpha*dW2\n",
    "            b2 = b2 - alpha*db2\n",
    "\n",
    "            W3 = W3 - alpha*dW3\n",
    "            b3 = b3 - alpha*db3\n",
    "\n",
    "            ##########Print error every 1000 Epochs#############\n",
    "            if epoch % 1000 == 0:\n",
    "                trainError,cvError = compute_final_errors(X, y, X_cv, y_cv, W1, b1, W2, b2, W3, b3)\n",
    "                print(f\"Epoch {epoch}, Train Error: {trainError:.4f}, Validation Error: {cvError:.4f}\")\n",
    "\n",
    "                 ############# Early stopping logic###############\n",
    "                if cvError + min_delta < best_cv_error:\n",
    "                    best_cv_error = cvError\n",
    "                    best_weights = (W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy())\n",
    "                    wait = 0\n",
    "                else:\n",
    "                    wait += 1000\n",
    "\n",
    "                if wait >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} for lambda {_lambda:.5f}. Best validation error: {best_cv_error:.4f}\")\n",
    "                    break\n",
    "        if best_weights:\n",
    "            W1, b1, W2, b2, W3, b3 = best_weights\n",
    "            \n",
    "        trainError,cvError = compute_final_errors(X, y, X_cv, y_cv, W1, b1, W2, b2, W3, b3)\n",
    "        print(f\"\\nlamdba:{_lambda:.5f}\")\n",
    "        print(f\"Train Error: {trainError:.4f}\")\n",
    "        print(f\"Cross validation Error: {cvError:.4f} \\n\")\n",
    "        print(\"***\"*15,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb7a4a9-0b40-4e40-bcc0-a14e907bf43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Error: 48.4751, Validation Error: 45.0883\n",
      "Epoch 1000, Train Error: 0.3630, Validation Error: 0.6885\n",
      "Epoch 2000, Train Error: 0.2087, Validation Error: 0.5156\n",
      "Epoch 3000, Train Error: 0.1527, Validation Error: 0.4313\n",
      "Epoch 4000, Train Error: 0.1231, Validation Error: 0.3772\n",
      "Epoch 5000, Train Error: 0.1047, Validation Error: 0.3342\n",
      "Epoch 6000, Train Error: 0.0932, Validation Error: 0.3059\n",
      "Epoch 7000, Train Error: 0.0779, Validation Error: 0.2731\n",
      "Epoch 8000, Train Error: 0.0676, Validation Error: 0.2581\n",
      "Epoch 9000, Train Error: 0.0584, Validation Error: 0.2479\n",
      "\n",
      "lamdba:0.00001\n",
      "Train Error: 0.0584\n",
      "Cross validation Error: 0.2479 \n",
      "\n",
      "********************************************* \n",
      "\n",
      "Epoch 0, Train Error: 48.4751, Validation Error: 45.0883\n",
      "Epoch 1000, Train Error: 0.3987, Validation Error: 0.2978\n",
      "Epoch 2000, Train Error: 0.2318, Validation Error: 0.1882\n",
      "Epoch 3000, Train Error: 0.1606, Validation Error: 0.1513\n",
      "Epoch 4000, Train Error: 0.1273, Validation Error: 0.1270\n",
      "Epoch 5000, Train Error: 0.1086, Validation Error: 0.1158\n",
      "Epoch 6000, Train Error: 0.0991, Validation Error: 0.1045\n",
      "Epoch 7000, Train Error: 0.0803, Validation Error: 0.1004\n",
      "Epoch 8000, Train Error: 0.0681, Validation Error: 0.1006\n",
      "Epoch 9000, Train Error: 0.0601, Validation Error: 0.1017\n",
      "\n",
      "lamdba:0.00010\n",
      "Train Error: 0.0803\n",
      "Cross validation Error: 0.1004 \n",
      "\n",
      "********************************************* \n",
      "\n",
      "Epoch 0, Train Error: 48.4751, Validation Error: 45.0883\n",
      "Epoch 1000, Train Error: 0.3619, Validation Error: 0.6862\n",
      "Epoch 2000, Train Error: 0.2174, Validation Error: 0.5360\n",
      "Epoch 3000, Train Error: 0.1800, Validation Error: 0.4975\n",
      "Epoch 4000, Train Error: 0.1230, Validation Error: 0.3748\n",
      "Epoch 5000, Train Error: 0.1052, Validation Error: 0.3343\n",
      "Epoch 6000, Train Error: 0.0930, Validation Error: 0.3056\n",
      "Epoch 7000, Train Error: 0.0790, Validation Error: 0.2769\n",
      "Epoch 8000, Train Error: 0.0648, Validation Error: 0.2517\n",
      "Epoch 9000, Train Error: 0.0579, Validation Error: 0.2459\n",
      "\n",
      "lamdba:0.00100\n",
      "Train Error: 0.0579\n",
      "Cross validation Error: 0.2459 \n",
      "\n",
      "********************************************* \n",
      "\n",
      "Epoch 0, Train Error: 48.4751, Validation Error: 45.0883\n",
      "Epoch 1000, Train Error: 0.3822, Validation Error: 0.2781\n",
      "Epoch 2000, Train Error: 0.1768, Validation Error: 0.1804\n",
      "Epoch 3000, Train Error: 0.1414, Validation Error: 0.1507\n",
      "Epoch 4000, Train Error: 0.1419, Validation Error: 0.1240\n",
      "Epoch 5000, Train Error: 0.1218, Validation Error: 0.1191\n",
      "Epoch 6000, Train Error: 0.1119, Validation Error: 0.1153\n",
      "Epoch 7000, Train Error: 0.0989, Validation Error: 0.1187\n",
      "Epoch 8000, Train Error: 0.0838, Validation Error: 0.1249\n",
      "Epoch 9000, Train Error: 0.0825, Validation Error: 0.1228\n",
      "Early stopping at epoch 9000 for lambda 0.01000. Best validation error: 0.1153\n",
      "\n",
      "lamdba:0.01000\n",
      "Train Error: 0.1119\n",
      "Cross validation Error: 0.1153 \n",
      "\n",
      "********************************************* \n",
      "\n",
      "Epoch 0, Train Error: 48.4752, Validation Error: 45.0884\n",
      "Epoch 1000, Train Error: 0.3143, Validation Error: 0.3158\n",
      "Epoch 2000, Train Error: 0.2089, Validation Error: 0.2112\n",
      "Epoch 3000, Train Error: 0.1364, Validation Error: 0.1465\n",
      "Epoch 4000, Train Error: 0.1035, Validation Error: 0.1336\n",
      "Epoch 5000, Train Error: 0.0921, Validation Error: 0.1212\n",
      "Epoch 6000, Train Error: 0.0793, Validation Error: 0.1148\n",
      "Epoch 7000, Train Error: 0.0671, Validation Error: 0.1102\n",
      "Epoch 8000, Train Error: 0.0517, Validation Error: 0.1089\n",
      "Epoch 9000, Train Error: 0.0458, Validation Error: 0.1065\n",
      "\n",
      "lamdba:0.10000\n",
      "Train Error: 0.0458\n",
      "Cross validation Error: 0.1065 \n",
      "\n",
      "********************************************* \n",
      "\n"
     ]
    }
   ],
   "source": [
    "_lambda_arr = np.array([0.00001,0.0001,0.001,0.01,0.1])\n",
    "alpha = 0.0001\n",
    "training_model_lambda(_lambda_arr,alpha,X_train_scaled,y_train,X_cv_scaled,y_cv,10000)\n",
    "#lambda just right = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0291a8b2-a872-4c9f-bc33-cbfe52bb5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_alpha(alpha_arr,_lambda,X,y,X_cv,y_cv,epochs,patience=3000, min_delta=1e-4):\n",
    "    for alpha in alpha_arr:\n",
    "        W1,b1,W2,b2,W3,b3 = weight_bias_initialize()\n",
    "        best_cv_error = float(\"inf\")\n",
    "        best_weights = None\n",
    "        wait = 0\n",
    "        for epoch in range(epochs):\n",
    "    \n",
    "            ##########Forward propagation##########\n",
    "            z1 = X @ W1 + b1 #(2076,5)@(5,64)\n",
    "            a1 = relu(z1)\n",
    "\n",
    "            z2 = a1 @ W2 + b2 # (2076,64)@(64,32)\n",
    "            a2 = relu(z2)\n",
    "\n",
    "            z3 = a2 @ W3 + b3 #(2076,32)@(32,1)\n",
    "            a3 = linear(z3)\n",
    "\n",
    "            error = np.mean((a3 - y)**2 ) + _lambda * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n",
    "            ##########Backward propagation#############\n",
    "\n",
    "            dz3 = a3 - y\n",
    "            dW3 = a2.T @ dz3 + _lambda * W3\n",
    "            db3 = np.sum(dz3,axis=0,keepdims = True)\n",
    "\n",
    "            da2 = dz3 @ W3.T         \n",
    "            dz2 = da2 * relu_derivative(z2)  \n",
    "            dW2 = a1.T @ dz2 + _lambda * W2        \n",
    "            db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "            da1 = dz2 @ W2.T         \n",
    "            dz1 = da1 * relu_derivative(z1)  \n",
    "            dW1 = X.T @ dz1 + _lambda * W1        \n",
    "            db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "            ##########Update Weights####################\n",
    "            W1 = W1 - alpha*dW1\n",
    "            b1 = b1 - alpha*db1\n",
    "\n",
    "            W2 = W2 - alpha*dW2\n",
    "            b2 = b2 - alpha*db2\n",
    "\n",
    "            W3 = W3 - alpha*dW3\n",
    "            b3 = b3 - alpha*db3\n",
    "\n",
    "            ##########Print error every 1000 Epochs#############\n",
    "            if epoch % 1000 == 0:\n",
    "                trainError,cvError = compute_final_errors(X, y, X_cv, y_cv, W1, b1, W2, b2, W3, b3)\n",
    "                print(f\"Epoch {epoch}, Train Error: {trainError:.4f}, Validation Error: {cvError:.4f}\")\n",
    "            ############# Early stopping logic###############\n",
    "                if cvError + min_delta < best_cv_error:\n",
    "                    best_cv_error = cvError\n",
    "                    best_weights = (W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy())\n",
    "                    wait = 0\n",
    "                else:\n",
    "                    wait += 1000\n",
    "\n",
    "                if wait >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch} for lambda {_lambda:.5f}. Best validation error: {best_cv_error:.4f}\")\n",
    "                    break\n",
    "                    \n",
    "        if best_weights:\n",
    "            W1, b1, W2, b2, W3, b3 = best_weights\n",
    "        \n",
    "        trainError,cvError = compute_final_errors(X, y, X_cv, y_cv, W1, b1, W2, b2, W3, b3)\n",
    "        print(f\"\\nalpha:{alpha:.5f}\")\n",
    "        print(f\"Train Error: {trainError:.4f}\")\n",
    "        print(f\"Cross validation Error: {cvError:.4f} \\n\")\n",
    "        print(\"***\"*15,\"\\n\")\n",
    "    return (W1,b1,W2,b2,W3,b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e67fc180-a6e8-4ce3-aaed-a09df091c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Error: 48.4751, Validation Error: 45.0883\n",
      "Epoch 1000, Train Error: 0.3987, Validation Error: 0.2978\n",
      "Epoch 2000, Train Error: 0.2318, Validation Error: 0.1882\n",
      "Epoch 3000, Train Error: 0.1606, Validation Error: 0.1513\n",
      "Epoch 4000, Train Error: 0.1273, Validation Error: 0.1270\n",
      "Epoch 5000, Train Error: 0.1086, Validation Error: 0.1158\n",
      "Epoch 6000, Train Error: 0.0991, Validation Error: 0.1045\n",
      "Epoch 7000, Train Error: 0.0803, Validation Error: 0.1004\n",
      "Epoch 8000, Train Error: 0.0681, Validation Error: 0.1006\n",
      "Epoch 9000, Train Error: 0.0601, Validation Error: 0.1017\n",
      "\n",
      "alpha:0.00010\n",
      "Train Error: 0.0803\n",
      "Cross validation Error: 0.1004 \n",
      "\n",
      "********************************************* \n",
      "\n",
      "Epoch 0, Train Error: 68.0569, Validation Error: 64.0369\n",
      "Epoch 1000, Train Error: 0.2760, Validation Error: 0.5121\n",
      "Epoch 2000, Train Error: 0.2096, Validation Error: 0.1879\n",
      "Epoch 3000, Train Error: 0.1728, Validation Error: 0.2849\n",
      "Epoch 4000, Train Error: 0.1641, Validation Error: 0.1667\n",
      "Epoch 5000, Train Error: 0.1538, Validation Error: 0.2250\n",
      "Epoch 6000, Train Error: 0.1545, Validation Error: 0.1244\n",
      "Epoch 7000, Train Error: 0.1270, Validation Error: 0.1939\n",
      "Epoch 8000, Train Error: 0.1334, Validation Error: 0.3485\n",
      "Epoch 9000, Train Error: 0.1169, Validation Error: 0.2452\n",
      "Early stopping at epoch 9000 for lambda 0.00010. Best validation error: 0.1244\n",
      "\n",
      "alpha:0.00003\n",
      "Train Error: 0.1545\n",
      "Cross validation Error: 0.1244 \n",
      "\n",
      "********************************************* \n",
      "\n",
      "Epoch 0, Train Error: 74.2024, Validation Error: 69.9992\n",
      "Epoch 1000, Train Error: 0.3394, Validation Error: 0.3987\n",
      "Epoch 2000, Train Error: 0.2649, Validation Error: 0.3555\n",
      "Epoch 3000, Train Error: 0.2238, Validation Error: 0.3259\n",
      "Epoch 4000, Train Error: 0.1966, Validation Error: 0.3037\n",
      "Epoch 5000, Train Error: 0.1756, Validation Error: 0.2839\n",
      "Epoch 6000, Train Error: 0.1573, Validation Error: 0.2657\n",
      "Epoch 7000, Train Error: 0.1419, Validation Error: 0.2511\n",
      "Epoch 8000, Train Error: 0.1291, Validation Error: 0.2373\n",
      "Epoch 9000, Train Error: 0.1199, Validation Error: 0.2287\n",
      "\n",
      "alpha:0.00001\n",
      "Train Error: 0.1199\n",
      "Cross validation Error: 0.2287 \n",
      "\n",
      "********************************************* \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.26879633e-02,  1.07624682e-03,  6.80841481e-02,\n",
       "          2.06740517e-01, -2.73254002e-01,  2.45845239e-04,\n",
       "          1.64487870e-01,  1.59251852e-01, -1.58955596e-02,\n",
       "          1.26752742e-02, -7.87640038e-02, -2.10083968e-02,\n",
       "         -1.28422044e-02, -2.39657823e-01, -2.88973172e-01,\n",
       "         -2.92262059e-01, -2.04580756e-01, -7.79932065e-02,\n",
       "         -7.60773280e-02, -5.14711785e-01,  8.60625081e-02,\n",
       "         -2.12637887e-01, -1.12991010e-02, -1.74834163e-01,\n",
       "         -1.60711756e-01, -9.81031749e-03, -1.01760912e-01,\n",
       "          1.29924409e-02, -6.76409012e-02, -2.34972305e-01,\n",
       "         -7.88113430e-05,  1.77941879e-01,  2.91265622e-01,\n",
       "         -1.62623412e-01,  5.40420052e-02, -1.08615329e-01,\n",
       "         -3.61802332e-01, -1.48216843e-01, -1.43981074e-01,\n",
       "          1.86295364e-02,  5.49200011e-02, -2.94889636e-02,\n",
       "          3.13521300e-02, -9.78677716e-02, -1.83329437e-01,\n",
       "         -6.71239207e-02,  4.08496935e-02,  4.38400980e-02,\n",
       "          7.12350685e-02, -2.83313048e-01,  1.77001414e-02,\n",
       "         -1.17820328e-01, -1.11290749e-02, -5.36766506e-01,\n",
       "          1.62866251e-01,  1.34130372e-01, -3.43793931e-01,\n",
       "          5.69881714e-02, -1.55428593e-01,  6.49459905e-02,\n",
       "          8.00455174e-02, -7.31867916e-02, -1.67876610e-01,\n",
       "         -6.50099917e-02],\n",
       "        [ 4.66806313e-02,  9.76526939e-02,  3.58629142e-02,\n",
       "          8.63899585e-02,  1.44330559e-01, -1.33305082e-01,\n",
       "          9.86478303e-02,  3.11328227e-01, -1.49291698e-01,\n",
       "          1.65234101e-01, -4.05052588e-01,  1.53438146e-02,\n",
       "          6.01648999e-02,  2.17480565e-02,  1.36944386e-01,\n",
       "         -3.58016044e-01, -4.66945577e-02, -1.32875317e-01,\n",
       "          1.24391765e-01, -3.21430184e-01, -3.26120636e-01,\n",
       "         -1.16997040e-01,  8.30835251e-02,  2.27666729e-02,\n",
       "         -1.86643690e-01, -3.18183360e-02, -2.14620595e-02,\n",
       "          1.01280438e-01, -1.22458279e-01,  1.99259729e-02,\n",
       "         -1.19705601e-01, -2.28570855e-01, -2.69264476e-02,\n",
       "         -8.92163693e-03, -8.66779955e-02, -1.09434838e-01,\n",
       "         -1.45586024e-01, -7.53382687e-02,  1.36297845e-02,\n",
       "         -4.23159798e-02, -1.12087773e-02, -9.29981425e-02,\n",
       "          3.12554627e-01,  1.13475573e-01,  8.03456207e-02,\n",
       "         -1.89284553e-02, -6.63958563e-02, -3.68762627e-02,\n",
       "          8.95001332e-02,  1.98770631e-01, -5.58335151e-03,\n",
       "         -3.15353453e-03, -3.03308685e-02, -1.26644258e-01,\n",
       "          7.55510487e-02,  9.49917463e-02,  3.09152999e-02,\n",
       "         -9.53533660e-03,  1.97829825e-01, -1.74361290e-01,\n",
       "          2.02817804e-01,  3.75263330e-01, -1.88356073e-01,\n",
       "         -1.18753276e-01],\n",
       "        [-3.79633915e-02, -2.65364021e-03, -1.81322730e-01,\n",
       "          9.77657373e-02, -1.05362812e-01,  8.62547354e-02,\n",
       "         -1.70576649e-01,  3.95230694e-01, -1.90510229e-01,\n",
       "          3.59025856e-02,  1.76706757e-01, -2.03871828e-01,\n",
       "         -2.80860014e-02,  1.87085584e-01, -2.56920226e-01,\n",
       "         -1.28850476e-01, -3.49701857e-02,  1.59763820e-01,\n",
       "          4.35333013e-02, -2.94572559e-03,  1.19451352e-01,\n",
       "         -9.59600886e-02,  3.29874882e-02, -3.86769192e-03,\n",
       "         -2.98144363e-01,  3.64026474e-02, -2.83497269e-03,\n",
       "         -6.09758838e-02,  2.69116040e-01,  1.06368514e-01,\n",
       "         -2.50415358e-01,  1.28048811e-01, -3.88701048e-01,\n",
       "          8.12776681e-02,  1.27147839e-01, -2.00415836e-01,\n",
       "         -2.27686092e-01,  5.90332745e-02,  1.51388285e-01,\n",
       "          1.71403862e-01, -6.00326333e-02, -2.13980751e-01,\n",
       "         -1.32706870e-01, -1.92304275e-01, -1.55116930e-01,\n",
       "          9.27415111e-03,  2.71845219e-01,  7.56669942e-02,\n",
       "         -2.91531741e-01,  3.01097692e-01,  2.17839648e-02,\n",
       "          3.38732105e-01,  2.20100260e-01, -2.16640995e-01,\n",
       "         -8.99536477e-02,  4.47588859e-02, -2.65156466e-01,\n",
       "          2.79791982e-01,  4.47138744e-01,  1.22989378e-02,\n",
       "         -3.62361047e-02, -2.00421182e-01,  3.79294714e-02,\n",
       "          6.46031077e-02],\n",
       "        [-5.16620793e-02, -8.51134461e-02, -1.52579736e-02,\n",
       "          5.74221929e-02, -1.77562504e-01, -5.42042236e-02,\n",
       "         -6.91017198e-02, -1.80008008e-01,  1.07826737e-01,\n",
       "          2.88987200e-01,  6.44419419e-02,  2.22378695e-01,\n",
       "         -6.41799728e-02, -4.78411931e-02,  1.06422867e-01,\n",
       "          1.57613272e-01,  6.62635805e-02,  5.13362636e-01,\n",
       "          2.02015793e-01,  4.82262531e-01,  7.11258106e-02,\n",
       "          3.24393766e-01, -4.77222089e-02,  1.17539534e-01,\n",
       "         -5.50624306e-02, -7.73310109e-03, -1.88356094e-01,\n",
       "         -2.97581461e-02,  1.05686320e-01, -2.22993858e-01,\n",
       "          2.56232112e-01, -1.24093478e-01,  1.75212430e-01,\n",
       "          1.49631705e-01, -3.70697422e-02, -4.00034424e-02,\n",
       "         -2.21658292e-01,  8.07028940e-02, -2.20092713e-01,\n",
       "         -9.09081921e-02,  1.56448653e-02, -1.02737097e-01,\n",
       "          5.58047413e-01,  3.49358216e-02, -3.24880658e-01,\n",
       "          3.36269623e-02, -1.57154863e-01,  9.76120496e-02,\n",
       "         -2.91492632e-01, -6.86706829e-03,  3.15041351e-02,\n",
       "         -1.12452229e-01, -2.38714558e-01, -1.85460755e-01,\n",
       "         -9.98568356e-02, -2.89940946e-01,  6.61206802e-01,\n",
       "         -1.32705627e-01, -4.57664082e-01,  3.90005951e-02,\n",
       "          4.63658524e-01,  1.77162146e-01, -4.60236036e-02,\n",
       "         -5.67653305e-02],\n",
       "        [ 1.27185289e-01, -6.59518273e-02,  4.24293461e-02,\n",
       "         -8.50457944e-02, -1.22810560e-01, -5.56011611e-02,\n",
       "         -3.02727448e-01,  2.40139038e-02,  4.13380369e-03,\n",
       "         -1.88436324e-01, -1.01186234e-01, -1.81760382e-01,\n",
       "         -5.13573077e-02,  5.43001905e-03,  3.81574372e-01,\n",
       "         -1.83667355e-01,  2.39150621e-01,  1.05868696e-01,\n",
       "         -7.27179082e-02, -2.96535061e-01, -1.16370231e-01,\n",
       "         -1.56347755e-01, -3.34666868e-02, -1.10230307e-02,\n",
       "         -1.29275789e-01,  2.36950574e-02,  2.83270849e-01,\n",
       "         -1.20022939e-01,  1.81787768e-01, -1.88971368e-01,\n",
       "          3.86393891e-02, -1.75823244e-02, -6.86660725e-02,\n",
       "         -7.16066398e-02, -1.08026130e-02, -7.85680196e-02,\n",
       "         -9.00929953e-03,  1.70561346e-01,  1.31093108e-01,\n",
       "         -8.22702700e-02,  1.49615744e-01,  1.53156350e-01,\n",
       "          1.56852326e-02,  2.45961027e-01,  2.30165289e-01,\n",
       "         -7.56104195e-02,  1.21557071e-01,  4.81151337e-02,\n",
       "          7.61537610e-02, -3.94921942e-02,  1.88589541e-01,\n",
       "         -2.22261272e-01,  9.78500155e-02,  3.11640953e-01,\n",
       "         -4.20918849e-02,  1.94587084e-01,  3.19881766e-01,\n",
       "          9.73220470e-02,  1.38348841e-01, -8.79473354e-02,\n",
       "         -1.48567671e-01,  1.57881216e-01,  7.35242973e-02,\n",
       "         -1.03247070e-01]]),\n",
       " array([[ 0.10374109,  0.15504976,  0.04381929,  0.14154278, -0.00201921,\n",
       "          0.0283331 , -0.09611982,  0.10755523,  0.10022891,  0.18480687,\n",
       "          0.08716756,  0.15912188,  0.15427104,  0.07570385,  0.19509921,\n",
       "          0.10736472,  0.03748303,  0.09217918,  0.16332554, -0.16233984,\n",
       "          0.00719406,  0.12017486,  0.0647251 ,  0.1062922 ,  0.43343628,\n",
       "          0.12081906, -0.10743679,  0.07874468,  0.16668296,  0.01029356,\n",
       "          0.16192358,  0.15407481, -0.04858768,  0.08301141,  0.05085617,\n",
       "          0.30716636, -0.3701638 , -0.15232945, -0.1192818 , -0.1330493 ,\n",
       "          0.02973215,  0.14439804,  0.08151564,  0.12076629,  0.36262157,\n",
       "          0.04536156,  0.2374117 ,  0.07097169, -0.26342918,  0.00126477,\n",
       "         -0.0029338 ,  0.15843073,  0.05495317, -0.13487584,  0.10571619,\n",
       "         -0.11961419,  0.13862626,  0.18864313, -0.3071942 , -0.00643185,\n",
       "          0.1095845 ,  0.23104879,  0.17984207,  0.04162983]]),\n",
       " array([[ 0.00880839,  0.07823174, -0.0779137 , ..., -0.01384828,\n",
       "          0.03906865,  0.15504253],\n",
       "        [ 0.07992144,  0.016061  ,  0.0062873 , ..., -0.04738995,\n",
       "         -0.20048025, -0.05043372],\n",
       "        [-0.07694222,  0.03989805,  0.03315027, ...,  0.14901615,\n",
       "          0.04035043, -0.07259507],\n",
       "        ...,\n",
       "        [-0.06742595, -0.24905265, -0.11380787, ..., -0.06826031,\n",
       "         -0.10359889,  0.0064817 ],\n",
       "        [ 0.20405928,  0.36131877,  0.05687114, ..., -0.04365342,\n",
       "          0.10827948,  0.07332159],\n",
       "        [ 0.0446424 ,  0.09080542,  0.0031093 , ...,  0.13120462,\n",
       "         -0.03010693, -0.16205213]]),\n",
       " array([[ 0.00805687,  0.55333935,  0.00776479,  0.00257963,  0.03263918,\n",
       "          0.11144721,  0.244112  ,  0.33435889, -0.02972145, -0.06308626,\n",
       "          0.02586689,  0.24379611, -0.06952272,  0.01624014,  0.22195181,\n",
       "          0.01184889,  0.07639026, -0.01597295,  0.05447022, -0.04062907,\n",
       "          0.32888471,  0.3670059 , -0.02384137, -0.04679353, -0.03554569,\n",
       "         -0.03386459,  0.0386591 , -0.07735476,  0.01236673, -0.03444897,\n",
       "          0.1885114 ,  0.02777433]]),\n",
       " array([[-0.1635299 ],\n",
       "        [ 1.84500174],\n",
       "        [ 0.1152262 ],\n",
       "        [-0.06066779],\n",
       "        [ 0.28054004],\n",
       "        [ 0.57166603],\n",
       "        [ 0.6072412 ],\n",
       "        [ 1.02649202],\n",
       "        [-0.09043191],\n",
       "        [-0.09379867],\n",
       "        [-0.06288971],\n",
       "        [ 0.87477386],\n",
       "        [-0.08707211],\n",
       "        [-0.28896754],\n",
       "        [ 0.89762333],\n",
       "        [ 0.16514074],\n",
       "        [ 0.32586612],\n",
       "        [-0.10220384],\n",
       "        [-0.14008728],\n",
       "        [-0.21876742],\n",
       "        [ 0.88313831],\n",
       "        [ 1.07957168],\n",
       "        [-0.18028572],\n",
       "        [-0.08118956],\n",
       "        [-0.21639545],\n",
       "        [-0.21793795],\n",
       "        [-0.09400686],\n",
       "        [-0.19356857],\n",
       "        [-0.20303892],\n",
       "        [-0.33682468],\n",
       "        [ 0.45288471],\n",
       "        [ 0.18229225]]),\n",
       " array([[1.54159146]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_arr = np.array([0.0001,0.00003,0.00001])\n",
    "_lambda = 0.0001\n",
    "training_model_alpha(alpha_arr,_lambda,X_train_scaled,y_train,X_cv_scaled,y_cv,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76f662ef-60db-49ee-bf5c-2346a3d6d82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Error: 48.4751, Validation Error: 45.0883\n",
      "Epoch 1000, Train Error: 0.3987, Validation Error: 0.2978\n",
      "Epoch 2000, Train Error: 0.2318, Validation Error: 0.1882\n",
      "Epoch 3000, Train Error: 0.1606, Validation Error: 0.1513\n",
      "Epoch 4000, Train Error: 0.1273, Validation Error: 0.1270\n",
      "Epoch 5000, Train Error: 0.1086, Validation Error: 0.1158\n",
      "Epoch 6000, Train Error: 0.0991, Validation Error: 0.1045\n",
      "Epoch 7000, Train Error: 0.0803, Validation Error: 0.1004\n",
      "Epoch 8000, Train Error: 0.0681, Validation Error: 0.1006\n",
      "Epoch 9000, Train Error: 0.0601, Validation Error: 0.1017\n",
      "\n",
      "alpha:0.00010\n",
      "Train Error: 0.0803\n",
      "Cross validation Error: 0.1004 \n",
      "\n",
      "********************************************* \n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha_arr = np.array([0.0001])\n",
    "_lambda = 0.0001\n",
    "W1,b1,W2,b2,W3,b3 = training_model_alpha(alpha_arr,_lambda,X_train_scaled,y_train,X_cv_scaled,y_cv,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e172ec-c506-421a-9ae9-8359056f92ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_neuron_network(X,W1,b1,W2,b2,W3,b3):\n",
    "    z1 = X @ W1 + b1 #(2076,5)@(5,64)\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    z2 = a1 @ W2 + b2 # (2076,64)@(64,32)\n",
    "    a2 = relu(z2)\n",
    "\n",
    "    z3 = a2 @ W3 + b3 #(2076,32)@(32,1)\n",
    "    a3 = linear(z3)\n",
    "    return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb53ef5a-47cd-4258-bac2-4236aa3abe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = my_neuron_network(X_test_scaled,W1,b1,W2,b2,W3,b3)\n",
    "\n",
    "MSE_test =  np.mean((y_predict - y_test)**2)\n",
    "r2_test = r2_score(y_test, y_predict)\n",
    "\n",
    "print(f\"MSE score:{MSE_test:.4f},r2 score:{r2_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b32c68-5bdc-4b11-ad45-4014ec0733d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42db27-7bd2-43c3-a582-96faf6939b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
